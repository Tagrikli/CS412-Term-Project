{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/embeddings/distilbert/uc_em_mean.parquet\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from consts import *\n",
    "\n",
    "\n",
    "df_embeds = pd.read_parquet(get_filename(TT.ALL, EMBED_PROVIDER.DISTILBERT, MEAN_METHOD.CENTROID))\n",
    "df_train_all = pd.read_parquet('data/training-dataset.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tagrikli/Desktop/Academic/Courses/CS412   - Machine Learning/Term Project/Data Modelling/env/lib/python3.9/site-packages/sklearn/model_selection/_split.py:805: UserWarning: The least populated class in y has only 4 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder, TargetEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "df_train = pd.merge(df_embeds, df_train_all[['username','category_enum','label']], how='left', on='username')\n",
    "df_train = df_train.dropna(subset=['label'])\n",
    "df_train = df_train.drop(columns=['username'])\n",
    "df_train['category_enum'] = df_train['category_enum'].fillna('Missing')\n",
    "\n",
    "\n",
    "\n",
    "labels = ['entertainment', 'food', 'travel', 'health and lifestyle', 'mom and children', 'fashion', 'tech', 'sports', 'art', 'gaming']\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "label_encoder.fit(labels)\n",
    "\n",
    "df_train['target'] = label_encoder.transform(df_train['label'])\n",
    "df_train = df_train.drop(columns=['label'])\n",
    "\n",
    "\n",
    "X = df_train.drop(columns=['target'])\n",
    "y = df_train[['target']]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=42)\n",
    "\n",
    "\n",
    "encoder = TargetEncoder()\n",
    "cat_encoded_train = encoder.fit_transform(X_train[\"category_enum\"].to_numpy().reshape(-1,1), y_train['target'].tolist())\n",
    "X_train['category_encoded'] = [row.tolist() for row in cat_encoded_train]\n",
    "X_train = X_train[['embeds_wavg','category_encoded']]\n",
    "\n",
    "X_train = pd.concat([pd.DataFrame(X_train['category_encoded'].tolist()), pd.DataFrame(X_train['embeds_wavg'].tolist())],axis=1)\n",
    "\n",
    "\n",
    "\n",
    "cat_encoded_test = encoder.transform(X_test['category_enum'].to_numpy().reshape(-1,1))\n",
    "X_test['category_encoded'] = [row.tolist() for row in cat_encoded_test]\n",
    "X_test = pd.concat([pd.DataFrame(X_test['category_encoded'].tolist()), pd.DataFrame(X_test['embeds_wavg'].tolist())],axis=1)\n",
    "\n",
    "X_train, X_test, y_train, y_test = X_train.to_numpy(), X_test.to_numpy(), y_train.to_numpy(), y_test.to_numpy()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<lightgbm.basic.Dataset object at 0x75f0f0249eb0>\n",
      "[LightGBM] [Warning] Unknown parameter: n_estimator\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=5, min_child_samples=5 will be ignored. Current value: min_data_in_leaf=5\n",
      "[LightGBM] [Warning] Unknown parameter: n_estimator\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=5, min_child_samples=5 will be ignored. Current value: min_data_in_leaf=5\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.035694 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 131238\n",
      "[LightGBM] [Info] Number of data points in the train set: 1370, number of used features: 522\n",
      "[LightGBM] [Warning] Unknown parameter: n_estimator\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=5, min_child_samples=5 will be ignored. Current value: min_data_in_leaf=5\n",
      "[LightGBM] [Info] Start training from score -2.700777\n",
      "[LightGBM] [Info] Start training from score -2.110578\n",
      "[LightGBM] [Info] Start training from score -2.232133\n",
      "[LightGBM] [Info] Start training from score -1.725398\n",
      "[LightGBM] [Info] Start training from score -5.836272\n",
      "[LightGBM] [Info] Start training from score -1.658046\n",
      "[LightGBM] [Info] Start training from score -2.840539\n",
      "[LightGBM] [Info] Start training from score -3.290740\n",
      "[LightGBM] [Info] Start training from score -2.024069\n",
      "[LightGBM] [Info] Start training from score -2.252753\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tagrikli/Desktop/Academic/Courses/CS412   - Machine Learning/Term Project/Data Modelling/env/lib/python3.9/site-packages/lightgbm/basic.py:357: UserWarning: Converting column-vector to 1d array\n",
      "  _log_warning(\"Converting column-vector to 1d array\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([5, 5, 8, ..., 3, 5, 3])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from lightgbm import LGBMClassifier, Dataset, train\n",
    "\n",
    "train_data = Dataset(X_train, label=y_train)\n",
    "test_data = Dataset(X_test, label=y_test)\n",
    "\n",
    "print(test_data)\n",
    "\n",
    "params = {\n",
    "    'objective':'multiclass',\n",
    "    'num_class':10,\n",
    "    'n_estimator':200,\n",
    "    'learning_rate':0.1,\n",
    "    'random_state':42,\n",
    "    'min_child_samples': 5,  # Try a smaller value (default is 20)\n",
    "    'min_data_in_leaf': 5    # Similar parameter, also try adjusting\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "#model = LGBMClassifier(\n",
    "#    objective='multiclass',\n",
    "#    num_class=10,\n",
    "#    n_estimators=200,\n",
    "#    learning_rate=0.1,\n",
    "#    random_state=42,\n",
    "#)\n",
    "\n",
    "#model = XGBClassifier(\n",
    "#    objective='multi:softmax',\n",
    "#    num_class=10,\n",
    "#    learning_rate=0.01,  # Lower learning rate\n",
    "#    max_depth=8,         # Try deeper trees\n",
    "#    min_child_weight=3,  # Increase to prevent overfitting\n",
    "#    n_estimators=200,    # More trees\n",
    "#    subsample=0.8,       # Add some randomness\n",
    "#    colsample_bytree=0.8,# Feature sampling\n",
    "#    eval_metrix=['mlogloss','merror'],\n",
    "#    early_stopping_rounds = 10,\n",
    "#    random_state=42\n",
    "#)\n",
    "\n",
    "bst = train(params, train_data, 10, valid_sets=[test_data])\n",
    "\n",
    "\n",
    "y_pred = bst.predict(X_test)\n",
    "#y_pred = model.predict(X_test)\n",
    "y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "y_pred_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Detailed Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.47      0.23      0.31        99\n",
      "           1       0.40      0.38      0.39       156\n",
      "           2       0.66      0.61      0.63       152\n",
      "           3       0.85      0.93      0.89       267\n",
      "           4       0.00      0.00      0.00         9\n",
      "           5       0.53      0.70      0.60       241\n",
      "           6       0.60      0.36      0.45        69\n",
      "           7       0.79      0.66      0.72        62\n",
      "           8       0.65      0.70      0.67       165\n",
      "           9       0.69      0.59      0.63       150\n",
      "\n",
      "    accuracy                           0.63      1370\n",
      "   macro avg       0.56      0.52      0.53      1370\n",
      "weighted avg       0.63      0.63      0.62      1370\n",
      "\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Classification metrics can't handle a mix of multiclass and continuous-multioutput targets",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 11\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(classification_report(y_test, y_pred_classes))\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# Calculate additional metrics\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m accuracy \u001b[38;5;241m=\u001b[39m \u001b[43maccuracy_score\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m macro_f1 \u001b[38;5;241m=\u001b[39m f1_score(y_test, y_pred, average\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmacro\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     13\u001b[0m weighted_f1 \u001b[38;5;241m=\u001b[39m f1_score(y_test, y_pred, average\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mweighted\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/Desktop/Academic/Courses/CS412   - Machine Learning/Term Project/Data Modelling/env/lib/python3.9/site-packages/sklearn/utils/_param_validation.py:216\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    210\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    211\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m    212\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m    213\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m    214\u001b[0m         )\n\u001b[1;32m    215\u001b[0m     ):\n\u001b[0;32m--> 216\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    217\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[1;32m    219\u001b[0m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[1;32m    220\u001b[0m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[1;32m    221\u001b[0m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[1;32m    222\u001b[0m     msg \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\n\u001b[1;32m    223\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+ must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    224\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    225\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[1;32m    226\u001b[0m     )\n",
      "File \u001b[0;32m~/Desktop/Academic/Courses/CS412   - Machine Learning/Term Project/Data Modelling/env/lib/python3.9/site-packages/sklearn/metrics/_classification.py:227\u001b[0m, in \u001b[0;36maccuracy_score\u001b[0;34m(y_true, y_pred, normalize, sample_weight)\u001b[0m\n\u001b[1;32m    225\u001b[0m \u001b[38;5;66;03m# Compute accuracy for each possible representation\u001b[39;00m\n\u001b[1;32m    226\u001b[0m y_true, y_pred \u001b[38;5;241m=\u001b[39m attach_unique(y_true, y_pred)\n\u001b[0;32m--> 227\u001b[0m y_type, y_true, y_pred \u001b[38;5;241m=\u001b[39m \u001b[43m_check_targets\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    228\u001b[0m check_consistent_length(y_true, y_pred, sample_weight)\n\u001b[1;32m    230\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m y_type\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmultilabel\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m~/Desktop/Academic/Courses/CS412   - Machine Learning/Term Project/Data Modelling/env/lib/python3.9/site-packages/sklearn/metrics/_classification.py:107\u001b[0m, in \u001b[0;36m_check_targets\u001b[0;34m(y_true, y_pred)\u001b[0m\n\u001b[1;32m    104\u001b[0m     y_type \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmulticlass\u001b[39m\u001b[38;5;124m\"\u001b[39m}\n\u001b[1;32m    106\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(y_type) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m--> 107\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    108\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mClassification metrics can\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt handle a mix of \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m and \u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;124m targets\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m    109\u001b[0m             type_true, type_pred\n\u001b[1;32m    110\u001b[0m         )\n\u001b[1;32m    111\u001b[0m     )\n\u001b[1;32m    113\u001b[0m \u001b[38;5;66;03m# We can't have more than one value on y_type => The set is no more needed\u001b[39;00m\n\u001b[1;32m    114\u001b[0m y_type \u001b[38;5;241m=\u001b[39m y_type\u001b[38;5;241m.\u001b[39mpop()\n",
      "\u001b[0;31mValueError\u001b[0m: Classification metrics can't handle a mix of multiclass and continuous-multioutput targets"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, f1_score, precision_score, recall_score\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Print detailed metrics\n",
    "print(\"\\nDetailed Classification Report:\")\n",
    "print(classification_report(y_test, y_pred_classes))\n",
    "\n",
    "# Calculate additional metrics\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "macro_f1 = f1_score(y_test, y_pred, average='macro')\n",
    "weighted_f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "macro_precision = precision_score(y_test, y_pred, average='macro')\n",
    "macro_recall = recall_score(y_test, y_pred, average='macro')\n",
    "\n",
    "print(\"\\nAdditional Metrics:\")\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Macro F1: {macro_f1:.4f}\")\n",
    "print(f\"Weighted F1: {weighted_f1:.4f}\")\n",
    "print(f\"Macro Precision: {macro_precision:.4f}\")\n",
    "print(f\"Macro Recall: {macro_recall:.4f}\")\n",
    "\n",
    "# Create confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# Plot confusion matrix\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.ylabel('True Label')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.show()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
