{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Required Libraries\n",
    "Import the necessary libraries, including pandas, seaborn, and matplotlib.pyplot."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Required Libraries\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import date\n",
    "from datetime import datetime\n",
    "import holidays\n",
    "import numpy as np\n",
    "import demoji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('merged_final.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import holidays\n",
    "\n",
    "# Define Turkish holidays\n",
    "tr_holidays = holidays.Turkey()\n",
    "\n",
    "# Convert timestamp to datetime\n",
    "df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "\n",
    "# Apply the holiday check\n",
    "df['is_holiday'] = df['timestamp'].apply(lambda x: x in tr_holidays)\n",
    "\n",
    "# Extract additional date features\n",
    "df['day_of_week'] = df['timestamp'].dt.dayofweek\n",
    "df['month'] = df['timestamp'].dt.month\n",
    "df['year'] = df['timestamp'].dt.year\n",
    "df['hour'] = df['timestamp'].dt.hour\n",
    "# Sort DataFrame\n",
    "df = df.sort_values(['username', 'timestamp'], ascending=[True, True])\n",
    "\n",
    "# Calculate mean post count\n",
    "\n",
    "df['comments_count'].fillna(0, inplace=True)\n",
    "#  replace 0's with 1s\n",
    "df['comments_count'] = df['comments_count'].replace(0, 1)\n",
    "df['like_count'] = df['like_count'].replace(0, 1)\n",
    "df['like_count'].fillna(1, inplace=True)\n",
    "# Handle categorical data\n",
    "df['category_name'] = df['category_name'].fillna('Unknown')\n",
    "print(df['category_name'].value_counts())\n",
    "df['category_name'] = df['category_name'].astype('category').cat.codes\n",
    "df['follower_count'] = df['follower_count']\n",
    "# Handle hashtags, emojis, and tags\n",
    "df['hashtags_x'] = df['hashtags_x'].fillna(' ')\n",
    "df['tags'] = df['tags'].fillna(' ')\n",
    "df['hashtags_present_in_caption'] = df['hashtags_x'].apply(lambda x: 1 if len(x.split('#')) > 1 else 0)\n",
    "df['tags_present_in_caption'] = df['tags'].apply(lambda x: 1 if len(x.split('@')) > 1 else 0)\n",
    "df['count_caption'] = df['caption'].apply(lambda x: len(x.split(' ')))\n",
    "mean_post_count = df['post_count'].mean()\n",
    "df['post_count'] = df['post_count'].fillna(mean_post_count)\n",
    "# Drop unnecessary columns\n",
    "df = df.drop(['full_name'], axis=1)\n",
    "\n",
    "# Convert boolean columns to int\n",
    "df[df.select_dtypes(['bool']).columns] = df[df.select_dtypes(['bool']).columns].astype(int)\n",
    "df = df[df['hide_like_and_view_counts'] == 0]\n",
    "df = df.drop('hide_like_and_view_counts', axis=1)\n",
    "\n",
    "# Drop old separate media type columns if no longer needed\n",
    "df.drop(['media_type_IMAGE', 'media_type_CAROUSEL_ALBUM'], axis=1, inplace=True)\n",
    "\n",
    "# Sort columns by name\n",
    "df = df.reindex(sorted(df.columns), axis=1)\n",
    "\n",
    "# Save to CSV\n",
    "df.to_csv('numeric_data.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# Refined keyword list\n",
    "lottery_keywords = [\n",
    "    r\"\\bçekiliş(?:e|i|ler|ten|ler)?\\b\",  # Variations of çekiliş\n",
    "    r\"\\bhediye(?:ler|m)?\\b\",  # Variations of hediye\n",
    "    r\"\\bkatıl(?:mak|dım|ıyor)?\\b\",  # Variations of katıl\n",
    "    r\"\\bbol şans\\b\",  # Exact match for \"bol şans\"\n",
    "    r\"\\bödül(?:ler)?\\b\",  # Variations of ödül\n",
    "    r\"\\bkazanan(?:lar|ı)?\\b\",  # Variations of kazanan\n",
    "    r\"\\bsonuç(?:lar)?\\b\",  # Variations of sonuç\n",
    "    r\"\\bkazan(?:mak|dı|ıyor)?\\b\",  # Variations of kazan\n",
    "    r\"\\betiket(?:lemek|le)?\\b\",  # Variations of etiket and etiketlemek\n",
    "]\n",
    "\n",
    "# Add \"yorum\" as a separate condition\n",
    "mandatory_keyword = r\"\\byorum(?: bırak| yaz|lara|la)?\\b\"  # Variations of yorum actions\n",
    "\n",
    "# Create combined pattern\n",
    "lottery_pattern = f\"(?=.*{mandatory_keyword})(?=.*({'|'.join(lottery_keywords)}))\"\n",
    "\n",
    "# Apply the pattern to cleaned_caption\n",
    "df['is_lottery'] = df['cleaned_caption'].str.contains(lottery_pattern, case=False, na=False)\n",
    "\n",
    "# Function to replace high-quartile lottery comments\n",
    "def adjust_comments(group):\n",
    "    # Calculate Q3 for comments_count\n",
    "    Q3 = np.percentile(group['comments_count'], 75)\n",
    "    \n",
    "    # Identify high-quartile and lottery posts\n",
    "    high_quartile = group['comments_count'] > Q3\n",
    "    lottery_posts = group['is_lottery']\n",
    "    \n",
    "    # Calculate mean of non-lottery posts for replacement\n",
    "    mean_non_lottery = group.loc[~lottery_posts, 'comments_count'].mean()\n",
    "    \n",
    "    # Replace high-quartile lottery comments with mean_non_lottery\n",
    "    group.loc[high_quartile & lottery_posts, 'comments_count'] = mean_non_lottery\n",
    "    \n",
    "    return group\n",
    "\n",
    "# Group by user and adjust comments\n",
    "df = df.groupby('username').apply(adjust_comments)\n",
    "df.to_csv('lottery_fixed.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "df = pd.read_csv('lottery_fixed.csv')\n",
    "\n",
    "# List to store the usernames to drop\n",
    "users_to_drop = []\n",
    "#  drop users with fewer than 2 posts\n",
    "for username, group in df.groupby('username'):\n",
    "    if len(group) < 3:\n",
    "        users_to_drop.append(username)\n",
    "# Drop the users with fewer than 3 posts\n",
    "df = df[~df['username'].isin(users_to_drop)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Assuming the necessary functions (e.g., calculate_all_stats, create_t_features, etc.) are defined earlier\n",
    "\n",
    "# Function to add lagged features to the original dataset\n",
    "def add_lagged_features_to_dataset(df):\n",
    "    lagged_data = []\n",
    "    \n",
    "    # Iterate over each row in the dataset\n",
    "    for idx, row in df.iterrows():\n",
    "        # Get the user's group (all posts by the same user)\n",
    "        user_group = df[df['username'] == row['username']]\n",
    "\n",
    "        # Get all posts that are before the current post (this ensures the lags are valid)\n",
    "        lags = user_group[user_group['timestamp'] < row['timestamp']]\n",
    "\n",
    "        # Skip this row if there are no previous posts (lags)\n",
    "        if len(lags) == 0:\n",
    "            continue  # Skip to the next row\n",
    "\n",
    "        # Lagged features calculation for valid rows with lags\n",
    "        lagged_features = calculate_all_stats(lags, row)  # Lagged statistics\n",
    "        media_type_stats = calculate_media_type_stats(lags, row)  # Media type statistics\n",
    "        t_features = create_t_features(lags)  # Temporal features\n",
    "\n",
    "        # Combine the lagged features with the current row\n",
    "        modified_row = {\n",
    "            **row.to_dict(),  # Add the original row values\n",
    "            **lagged_features,  # Add lagged statistics\n",
    "            **media_type_stats,  # Add media type statistics\n",
    "            **t_features  # Add temporal features\n",
    "        }\n",
    "\n",
    "        # Append the modified row to the list\n",
    "        lagged_data.append(modified_row)\n",
    "    \n",
    "    # Convert the lagged data list into a DataFrame and return\n",
    "    return pd.DataFrame(lagged_data)\n",
    "\n",
    "\n",
    "# Function to apply outlier handling and generate lagged features (from your original code)\n",
    "def handle_outliers(lags, username, multiplier=100.0, column='like_count'):\n",
    "    if lags[column].max() > 100000 or (column == 'comments_count' and lags[column].max() > 1000):\n",
    "        q1 = lags[column].quantile(0.1)\n",
    "        q3 = lags[column].quantile(0.9)\n",
    "        iqr = q3 - q1\n",
    "\n",
    "        if iqr == 0:\n",
    "            lower_bound = lags[column].min() - 1\n",
    "            upper_bound = lags[column].max() + 1\n",
    "        else:\n",
    "            lower_bound = max(q1 - (multiplier * iqr), 0)\n",
    "            upper_bound = q3 + (multiplier * iqr)\n",
    "\n",
    "        non_outlier_mask = (lags[column] >= lower_bound) & (lags[column] <= upper_bound)\n",
    "        non_outlier_values = lags[column][non_outlier_mask]\n",
    "\n",
    "        if not non_outlier_values.empty:\n",
    "            mean_value = int(round(non_outlier_values.mean()))\n",
    "            max_value = non_outlier_values.max()\n",
    "            min_value = non_outlier_values.min()\n",
    "\n",
    "            if (non_outlier_values.nunique() == 1):\n",
    "                replacement_value = non_outlier_values.iloc[0]\n",
    "            else:\n",
    "                replacement_value = max_value\n",
    "        else:\n",
    "            mean_value = 0\n",
    "            replacement_value = 0\n",
    "\n",
    "        lags.loc[lags[column] > upper_bound, column] = max_value if (non_outlier_values.nunique() > 1) else replacement_value\n",
    "        lags.loc[lags[column] < lower_bound, column] = min_value if (non_outlier_values.nunique() > 1) else replacement_value\n",
    "\n",
    "        return lags\n",
    "    else:\n",
    "        return lags\n",
    "\n",
    "\n",
    "def separate_target_and_lags(group):\n",
    "    group = group.sort_values('timestamp', ascending=True)\n",
    "    target_row = group.iloc[-1]  # Last row is the target\n",
    "    lags = group.iloc[:-1]  # All but the last row are lags\n",
    "    return target_row, lags\n",
    "\n",
    "\n",
    "def calculate_all_stats(lags, target, suffix=''):\n",
    "    like_to_comment_ratio = lags['like_count'].mean() / (lags['comments_count'].mean() + 1e-5)\n",
    "    if target['comments_count'] == 0:\n",
    "        target['comments_count'] = lags['comments_count'].mean()\n",
    "    estimate = target['comments_count'] * like_to_comment_ratio\n",
    "\n",
    "    return {\n",
    "        'mean_likes' + suffix: lags['like_count'].mean(),\n",
    "        'max_likes' + suffix: lags['like_count'].max(),\n",
    "        'median_likes' + suffix: lags['like_count'].median(),\n",
    "        'min_likes' + suffix: lags['like_count'].min(),\n",
    "        'mean_comments' + suffix: lags['comments_count'].mean(),\n",
    "        'max_comments' + suffix: lags['comments_count'].max(),\n",
    "        'median_comments' + suffix: lags['comments_count'].median(),\n",
    "        'min_comments' + suffix: lags['comments_count'].min(),\n",
    "        'like_to_comment_ratio' + suffix: like_to_comment_ratio,\n",
    "        'estimate_likes' + suffix: estimate\n",
    "    }\n",
    "\n",
    "\n",
    "def calculate_media_type_stats(lags, target):\n",
    "    current_media_type = target['media_type_VIDEO']\n",
    "    videos_count = lags['media_type_VIDEO'].sum()\n",
    "    non_videos_count = len(lags) - videos_count\n",
    "    is_primarily_videoer = 1 if videos_count > non_videos_count else 0\n",
    "    media_type_lags = lags[lags['media_type_VIDEO'] == current_media_type].copy()\n",
    "\n",
    "    if len(media_type_lags) == 0 or len(media_type_lags) < 3:\n",
    "        media_type_lags = lags.copy()  # Fallback if no matching media type\n",
    "\n",
    "    like_to_comment_ratio = media_type_lags['like_count'].mean() / (media_type_lags['comments_count'].mean() + 1e-5)\n",
    "    if target['comments_count'] == 0:\n",
    "        target['comments_count'] = media_type_lags['comments_count'].mean()\n",
    "    estimate = target['comments_count'] * like_to_comment_ratio\n",
    "\n",
    "    return {\n",
    "        'is_primarily_videoer': is_primarily_videoer,\n",
    "        'mean_likes_media_type': media_type_lags['like_count'].mean(),\n",
    "        'max_likes_media_type': media_type_lags['like_count'].max(),\n",
    "        'median_likes_media_type': media_type_lags['like_count'].median(),\n",
    "        'min_likes_media_type': media_type_lags['like_count'].min(),\n",
    "        'mean_comments_media_type': media_type_lags['comments_count'].mean(),\n",
    "        'max_comments_media_type': media_type_lags['comments_count'].max(),\n",
    "        'median_comments_media_type': media_type_lags['comments_count'].median(),\n",
    "        'min_comments_media_type': media_type_lags['comments_count'].min(),\n",
    "        'estimate_likes_media_type': estimate\n",
    "    }\n",
    "\n",
    "\n",
    "def create_t_features(lags):\n",
    "    t_features = {}\n",
    "    for t in range(1, 34):\n",
    "        if len(lags) >= t:\n",
    "            t_features[f't_{t}_likes'] = lags.iloc[-t]['like_count']\n",
    "        else:\n",
    "            t_features[f't_{t}_likes'] = 0\n",
    "    return t_features\n",
    "\n",
    "\n",
    "# Example: Assuming 'df' is your original DataFrame\n",
    "df['timestamp'] = pd.to_datetime(df['timestamp'])  # Ensure timestamp is in datetime format\n",
    "grouped = df.groupby('username')\n",
    "\n",
    "# Now apply the function to add lagged features to the dataset\n",
    "df_with_lags = add_lagged_features_to_dataset(df)\n",
    "\n",
    "# Save the resulting DataFrame to a CSV file\n",
    "df_with_lags.to_csv('df_with_lags.csv', index=False)\n",
    "\n",
    "print(\"Lagged features added to dataset and saved as df_with_lags.csv.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_with_lags.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Dataset\n",
    "df = pd.read_csv('df_with_lags.csv')  # Replace with your dataset path\n",
    "columns_to_keep = [\n",
    "    'hashtags_x', \n",
    "    'cleaned_caption', \n",
    "    'caption', \n",
    "    'tags', \n",
    "    'Unnamed: 0', 'biography', 'id_x'\n",
    "]\n",
    "print(df[   'hashtags_x'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import demoji\n",
    "import math\n",
    "\n",
    "\n",
    "# Load the sentiment mapping table\n",
    "sentiment_df = pd.read_csv(r'tansu_emojis.csv')\n",
    "\n",
    "# Create a dictionary for quick lookup of sentiment scores\n",
    "sentiment_score_map = dict(zip(sentiment_df['Emoji'], sentiment_df['Score']))\n",
    "\n",
    "# Define a function to calculate weighted sentiment\n",
    "def calculate_weighted_sentiment(caption):\n",
    "    emojis = demoji.findall(caption)  # Find all emojis in the caption\n",
    "    if not emojis:\n",
    "        return 0  # Return 0 if no emojis are found\n",
    "    \n",
    "    total_weighted_sentiment = 0\n",
    "    total_count = 0\n",
    "    \n",
    "    # Calculate weighted sentiment score\n",
    "    for emoji, desc in emojis.items():\n",
    "        count = caption.count(emoji)\n",
    "        sentiment_score = sentiment_score_map.get(emoji)\n",
    "        \n",
    "        # Skip if sentiment score is NaN or None\n",
    "        if sentiment_score is None or math.isnan(sentiment_score):\n",
    "            continue\n",
    "        \n",
    "        total_weighted_sentiment += sentiment_score * count  # Multiply sentiment score by emoji count\n",
    "        total_count += count  # Add emoji count to total count\n",
    "    \n",
    "    # Return the weighted average sentiment score, or 0 if no valid emojis were found\n",
    "    return total_weighted_sentiment / total_count if total_count else 0\n",
    "\n",
    "# Process each caption and output a single column with the average sentiment\n",
    "df['sentiment'] = df['caption'].apply(\n",
    "    lambda caption: calculate_weighted_sentiment(caption) if isinstance(caption, str) else 0\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('sentiment_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv(\"sentiment_data.csv\")\n",
    "\n",
    "# Function to remove words of length less than 5\n",
    "def remove_short_words(text):\n",
    "    \"\"\"\n",
    "    Remove words with length less than 5 from a given text.\n",
    "    \"\"\"\n",
    "    return \" \".join([word for word in text.split() if len(word) >= 5])\n",
    "\n",
    "# Step 1: Fill NaN values and remove '#' from hashtags\n",
    "df[\"cleaned_caption\"] = df[\"cleaned_caption\"].fillna(\"\").apply(remove_short_words)\n",
    "df[\"hashtags_x\"] = df[\"hashtags_x\"].fillna(\"\").str.replace('#', '', regex=False).apply(remove_short_words)\n",
    "\n",
    "# Step 2: Apply TF-IDF Vectorization\n",
    "# Define the maximum number of features for captions and hashtags\n",
    "caption_tfidf_max_features = 300\n",
    "hashtag_tfidf_max_features = 100\n",
    "\n",
    "# TF-IDF Vectorizer for captions\n",
    "tfidf_caption = TfidfVectorizer(max_features=caption_tfidf_max_features)\n",
    "caption_tfidf_matrix = tfidf_caption.fit_transform(df[\"cleaned_caption\"])\n",
    "\n",
    "# TF-IDF Vectorizer for hashtags\n",
    "tfidf_hashtags = TfidfVectorizer(max_features=hashtag_tfidf_max_features)\n",
    "hashtags_tfidf_matrix = tfidf_hashtags.fit_transform(df[\"hashtags_x\"])\n",
    "\n",
    "# Step 3: Convert TF-IDF Matrices to DataFrames\n",
    "caption_tfidf_df = pd.DataFrame(caption_tfidf_matrix.toarray(), columns=tfidf_caption.get_feature_names_out())\n",
    "hashtags_tfidf_df = pd.DataFrame(hashtags_tfidf_matrix.toarray(), columns=tfidf_hashtags.get_feature_names_out())\n",
    "\n",
    "# Step 4: Combine TF-IDF Features with Original DataFrame\n",
    "df = pd.concat([df, caption_tfidf_df.add_prefix(\"caption_tfidf_\"), hashtags_tfidf_df.add_prefix(\"hashtags_tfidf_\")], axis=1)\n",
    "\n",
    "# Step 5: Verify Results\n",
    "print(df.head())\n",
    "\n",
    "# Save the resulting DataFrame (optional)\n",
    "df.to_csv(\"tfidf_transformed_data.csv\", index=False)\n",
    "print(\"TF-IDF transformed data saved to 'tfidf_transformed_data.csv'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the vectorizer after fitting it on training data\n",
    "import joblib\n",
    "joblib.dump(tfidf_caption, 'tfidf_vectorizer_captions.pkl')\n",
    "joblib.dump(tfidf_hashtags, 'tfidf_vectorizer_hashtags.pkl')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "import pandas as pd\n",
    "import zeyrek\n",
    "\n",
    "# Initialize Zeyrek MorphAnalyzer\n",
    "zeyrek = zeyrek.MorphAnalyzer()\n",
    "\n",
    "# Download necessary NLTK data\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Define the list of holiday-related words\n",
    "holiday_words = [\n",
    "    \"cumhuriyet\", \"kutlu\", \"türkiye\", \"bayram\", \"zafer\", \n",
    "    \"kurtuluş\", \"özgürlük\", \"şehit\", \"atatürk\", \"milli\", \"egemenlik\", \n",
    "    \"birlik\", \"beraberlik\",\n",
    "    \"anma\",  \"al bayrak\", \"100\", \"23\", \"millî egemenlik\", \n",
    "    \"23 nisan\", \"30 ağustos\", \"19 mayıs\", \"29 ekim\", \n",
    "    \"zafer bayramı\", \"cumhuriyet bayramı\", \"kutlu olsun\", \"kutlama\",\n",
    "    \"kutlamak\", \"anmak\", \"birleşmek\",\"başarmak\", \"kutlanmak\"\n",
    "]\n",
    "\n",
    "# Create a set for faster lookup\n",
    "holiday_words_set = set(holiday_words)\n",
    "\n",
    "# Function to check if any holiday word is in the cleaned hashtags\n",
    "def check_holiday_words(caption):\n",
    "    if not isinstance(caption, str):\n",
    "        return 0\n",
    "    words = caption.split()\n",
    "    return 1 if any(word in holiday_words_set for word in words) else 0\n",
    "\n",
    "# Create a new column to flag the presence of holiday words\n",
    "df['holiday_flag'] = df['cleaned_caption'].apply(check_holiday_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df['holiday_flag'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_keep = [\n",
    "    'hashtags_x', \n",
    "    'cleaned_caption', \n",
    "    'caption', \n",
    "    'tags', \n",
    "    'Unnamed: 0', 'biography', 'id_x'\n",
    "]\n",
    "df.drop(columns_to_keep, axis=1, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.select_dtypes(include=['object']).columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"last.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"last.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('columns.txt', 'w') as f:\n",
    "    for i in df.columns:\n",
    "        f.write(i + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['holiday_flag']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop([\"Unnamed: 0\"], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the ranges (min and max) of all columns in the dataframe, excluding \"timestamp\" and \"username\"\n",
    "for column in df.columns:\n",
    "    if column not in [\"timestamp\", \"username\"]:\n",
    "        min_value = df[column].min()\n",
    "        max_value = df[column].max()\n",
    "        # print(f\"Min: {min_value} | Max: {max_value}\")\n",
    "        if min_value < 0:\n",
    "            print(f\"Column: {column} | Min: {min_value} | Max: {max_value}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Plot distribution of the target variable\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(df['like_count'])\n",
    "plt.title('Distribution of Like Count')\n",
    "plt.xlabel('Like Count')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the distribution of categorical features\n",
    "categorical_columns = [\n",
    "    'category_name', 'is_verified', 'is_business_account', 'holiday_flag'\n",
    "]\n",
    "\n",
    "for col in categorical_columns:\n",
    "    print(f\"Distribution of {col}:\")\n",
    "    print(df[col].value_counts())\n",
    "    print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load your dataset\n",
    "df = pd.read_csv('last.csv')  # Example of loading your dataset\n",
    "df = df.drop([\"Unnamed: 0\", \"timestamp\", \"username\", \"category_name\"], axis=1)\n",
    "df['estimate_likes'] = np.maximum(df['estimate_likes'], df['min_likes'])\n",
    "df['estimate_likes_media_type'] = np.maximum(df['estimate_likes_media_type'], df['min_likes_media_type'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data shape: (160899, 474)\n",
      "like_count\n",
      "2.302585     2153\n",
      "2.079442     2138\n",
      "2.639057     2130\n",
      "2.197225     2102\n",
      "1.945910     2049\n",
      "             ... \n",
      "11.861693       1\n",
      "13.140728       1\n",
      "12.778452       1\n",
      "10.604057       1\n",
      "8.029759        1\n",
      "Name: count, Length: 17901, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler, LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Assuming df is already loaded\n",
    "\n",
    "# Step 1: Data Preparation\n",
    "print(\"Data shape:\", df.shape)\n",
    "\n",
    "# List of features that should be log-transformed\n",
    "log_transformed_features = [\n",
    "    'comments_count', 'follower_count', 'following_count', 'highlight_reel_count',\n",
    "    'like_count', 'post_count', 'mean_likes', 'max_likes', 'median_likes', \n",
    "    'min_likes', 'mean_comments', 'max_comments', 'median_comments', \n",
    "    'min_comments', 'estimate_likes', 'mean_likes_media_type', 'max_likes_media_type', \n",
    "    'median_likes_media_type', 'min_likes_media_type', 'mean_comments_media_type', \n",
    "    'max_comments_media_type', 'median_comments_media_type', 'min_comments_media_type', \n",
    "    'estimate_likes_media_type', 't_1_likes', 't_2_likes', 't_3_likes', 't_4_likes', \n",
    "    't_5_likes', 't_6_likes', 't_7_likes', 't_8_likes', 't_9_likes', 't_10_likes',\n",
    "    't_11_likes', 't_12_likes', 't_13_likes', 't_14_likes', 't_15_likes', 't_16_likes', \n",
    "    't_17_likes', 't_18_likes', 't_19_likes', 't_20_likes', 't_21_likes', 't_22_likes', \n",
    "    't_23_likes', 't_24_likes', 't_25_likes', 't_26_likes', 't_27_likes', 't_28_likes', \n",
    "    't_29_likes', 't_30_likes', 't_31_likes', 't_32_likes', 't_33_likes'\n",
    "]\n",
    "\n",
    "# Apply log transformation with a small epsilon to avoid log(0)\n",
    "for feature in log_transformed_features:\n",
    "    df[feature] = np.log(df[feature] + 1)\n",
    "print(df['like_count'].value_counts())\n",
    "\n",
    "# List of binary categorical features\n",
    "binary_features = ['is_verified', 'is_business_account', 'is_holiday', 'is_professional_account', 'is_lottery', 'media_type_VIDEO']\n",
    "\n",
    "# Encode binary categorical features using Label Encoding\n",
    "encoder = LabelEncoder()\n",
    "for feature in binary_features:\n",
    "    df[feature] = encoder.fit_transform(df[feature])\n",
    "\n",
    "# Apply MinMax scaling to all numerical features\n",
    "numerical_features = df.select_dtypes(include=['float64', 'int64']).columns  # Select all numerical features\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "df[numerical_features] = scaler.fit_transform(df[numerical_features])\n",
    "\n",
    "# Split the dataset into features and target\n",
    "X = df.drop(columns='like_count')  # Replace 'target_variable' with your target column name\n",
    "y = df['like_count']\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Now the dataset is preprocessed and ready for model training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "like_count\n",
      "0.104650    2153\n",
      "0.090141    2138\n",
      "0.126528    2130\n",
      "0.097799    2102\n",
      "0.081458    2049\n",
      "            ... \n",
      "0.726209       1\n",
      "0.809376       1\n",
      "0.785820       1\n",
      "0.644434       1\n",
      "0.477047       1\n",
      "Name: count, Length: 17901, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(df['like_count'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (128719,473) (474,) (128719,473) ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[39], line 21\u001b[0m\n\u001b[1;32m     17\u001b[0m y_test_original_log \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mexp(y_test) \u001b[38;5;241m-\u001b[39m epsilon\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# Now reverse the scaling applied to the features (Descaling)\u001b[39;00m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# For each feature, we need to reverse the scaling:\u001b[39;00m\n\u001b[0;32m---> 21\u001b[0m X_train_original \u001b[38;5;241m=\u001b[39m \u001b[43mscaler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minverse_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Descale X_train\u001b[39;00m\n\u001b[1;32m     22\u001b[0m X_test_original \u001b[38;5;241m=\u001b[39m scaler\u001b[38;5;241m.\u001b[39minverse_transform(X_test)  \u001b[38;5;66;03m# Descale X_test\u001b[39;00m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m# Here, we need to use the predicted and true values on their original scale (after descaling and log reversal)\u001b[39;00m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m# Calculate error metrics on the original scale\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/sklearn/preprocessing/_data.py:572\u001b[0m, in \u001b[0;36mMinMaxScaler.inverse_transform\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    563\u001b[0m xp, _ \u001b[38;5;241m=\u001b[39m get_namespace(X)\n\u001b[1;32m    565\u001b[0m X \u001b[38;5;241m=\u001b[39m check_array(\n\u001b[1;32m    566\u001b[0m     X,\n\u001b[1;32m    567\u001b[0m     copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy,\n\u001b[1;32m    568\u001b[0m     dtype\u001b[38;5;241m=\u001b[39m_array_api\u001b[38;5;241m.\u001b[39msupported_float_dtypes(xp),\n\u001b[1;32m    569\u001b[0m     force_all_finite\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow-nan\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    570\u001b[0m )\n\u001b[0;32m--> 572\u001b[0m X \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmin_\n\u001b[1;32m    573\u001b[0m X \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscale_\n\u001b[1;32m    574\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m X\n",
      "\u001b[0;31mValueError\u001b[0m: operands could not be broadcast together with shapes (128719,473) (474,) (128719,473) "
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "import numpy as np\n",
    "\n",
    "# Initialize the linear regression model\n",
    "model = LinearRegression()\n",
    "\n",
    "# Train the model using the training data\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set (log-transformed predictions)\n",
    "y_pred_log = model.predict(X_test)\n",
    "\n",
    "# Reverse the log transformation (use exp to get the original scale)\n",
    "epsilon = 1  # The value used in log(x + 1)\n",
    "y_pred_original_log = np.exp(y_pred_log) - epsilon\n",
    "y_test_original_log = np.exp(y_test) - epsilon\n",
    "\n",
    "# Now reverse the scaling applied to the features (Descaling)\n",
    "# For each feature, we need to reverse the scaling:\n",
    "X_train_original = scaler.inverse_transform(X_train)  # Descale X_train\n",
    "X_test_original = scaler.inverse_transform(X_test)  # Descale X_test\n",
    "\n",
    "# Here, we need to use the predicted and true values on their original scale (after descaling and log reversal)\n",
    "# Calculate error metrics on the original scale\n",
    "mae = mean_absolute_error(y_test_original_log, y_pred_original_log)\n",
    "mse = mean_squared_error(y_test_original_log, y_pred_original_log)\n",
    "rmse = np.sqrt(mse)\n",
    "r2 = r2_score(y_test_original_log, y_pred_original_log)\n",
    "\n",
    "# Output the evaluation metrics\n",
    "print(\"Model Evaluation Metrics on Original Scale (Descaled):\")\n",
    "print(f\"Mean Absolute Error (MAE): {mae}\")\n",
    "print(f\"Mean Squared Error (MSE): {mse}\")\n",
    "print(f\"Root Mean Squared Error (RMSE): {rmse}\")\n",
    "print(f\"R² (Coefficient of Determination): {r2}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data shape: (160899, 477)\n",
      "[0]\tvalidation_0-mae:2.61275\n",
      "[1]\tvalidation_0-mae:2.50837\n",
      "[2]\tvalidation_0-mae:2.40410\n",
      "[3]\tvalidation_0-mae:2.30034\n",
      "[4]\tvalidation_0-mae:2.19727\n",
      "[5]\tvalidation_0-mae:2.09534\n",
      "[6]\tvalidation_0-mae:1.99482\n",
      "[7]\tvalidation_0-mae:1.89603\n",
      "[8]\tvalidation_0-mae:1.79941\n",
      "[9]\tvalidation_0-mae:1.70527\n",
      "[10]\tvalidation_0-mae:1.61380\n",
      "[11]\tvalidation_0-mae:1.52529\n",
      "[12]\tvalidation_0-mae:1.44013\n",
      "[13]\tvalidation_0-mae:1.35850\n",
      "[14]\tvalidation_0-mae:1.28068\n",
      "[15]\tvalidation_0-mae:1.20676\n",
      "[16]\tvalidation_0-mae:1.13681\n",
      "[17]\tvalidation_0-mae:1.07083\n",
      "[18]\tvalidation_0-mae:1.00894\n",
      "[19]\tvalidation_0-mae:0.95129\n",
      "[20]\tvalidation_0-mae:0.89750\n",
      "[21]\tvalidation_0-mae:0.84762\n",
      "[22]\tvalidation_0-mae:0.80173\n",
      "[23]\tvalidation_0-mae:0.75955\n",
      "[24]\tvalidation_0-mae:0.72089\n",
      "[25]\tvalidation_0-mae:0.68549\n",
      "[26]\tvalidation_0-mae:0.65344\n",
      "[27]\tvalidation_0-mae:0.62434\n",
      "[28]\tvalidation_0-mae:0.59791\n",
      "[29]\tvalidation_0-mae:0.57414\n",
      "[30]\tvalidation_0-mae:0.55289\n",
      "[31]\tvalidation_0-mae:0.53397\n",
      "[32]\tvalidation_0-mae:0.51720\n",
      "[33]\tvalidation_0-mae:0.50233\n",
      "[34]\tvalidation_0-mae:0.48906\n",
      "[35]\tvalidation_0-mae:0.47753\n",
      "[36]\tvalidation_0-mae:0.46727\n",
      "[37]\tvalidation_0-mae:0.45839\n",
      "[38]\tvalidation_0-mae:0.45056\n",
      "[39]\tvalidation_0-mae:0.44386\n",
      "[40]\tvalidation_0-mae:0.43823\n",
      "[41]\tvalidation_0-mae:0.43320\n",
      "[42]\tvalidation_0-mae:0.42903\n",
      "[43]\tvalidation_0-mae:0.42541\n",
      "[44]\tvalidation_0-mae:0.42227\n",
      "[45]\tvalidation_0-mae:0.41958\n",
      "[46]\tvalidation_0-mae:0.41731\n",
      "[47]\tvalidation_0-mae:0.41538\n",
      "[48]\tvalidation_0-mae:0.41366\n",
      "[49]\tvalidation_0-mae:0.41204\n",
      "[50]\tvalidation_0-mae:0.41094\n",
      "[51]\tvalidation_0-mae:0.40987\n",
      "[52]\tvalidation_0-mae:0.40903\n",
      "[53]\tvalidation_0-mae:0.40832\n",
      "[54]\tvalidation_0-mae:0.40777\n",
      "[55]\tvalidation_0-mae:0.40721\n",
      "[56]\tvalidation_0-mae:0.40692\n",
      "[57]\tvalidation_0-mae:0.40651\n",
      "[58]\tvalidation_0-mae:0.40625\n",
      "[59]\tvalidation_0-mae:0.40571\n",
      "[60]\tvalidation_0-mae:0.40552\n",
      "[61]\tvalidation_0-mae:0.40524\n",
      "[62]\tvalidation_0-mae:0.40509\n",
      "[63]\tvalidation_0-mae:0.40499\n",
      "[64]\tvalidation_0-mae:0.40481\n",
      "[65]\tvalidation_0-mae:0.40464\n",
      "[66]\tvalidation_0-mae:0.40446\n",
      "[67]\tvalidation_0-mae:0.40438\n",
      "[68]\tvalidation_0-mae:0.40429\n",
      "[69]\tvalidation_0-mae:0.40416\n",
      "[70]\tvalidation_0-mae:0.40412\n",
      "[71]\tvalidation_0-mae:0.40401\n",
      "[72]\tvalidation_0-mae:0.40396\n",
      "[73]\tvalidation_0-mae:0.40384\n",
      "[74]\tvalidation_0-mae:0.40380\n",
      "[75]\tvalidation_0-mae:0.40376\n",
      "[76]\tvalidation_0-mae:0.40372\n",
      "[77]\tvalidation_0-mae:0.40363\n",
      "[78]\tvalidation_0-mae:0.40358\n",
      "[79]\tvalidation_0-mae:0.40350\n",
      "[80]\tvalidation_0-mae:0.40343\n",
      "[81]\tvalidation_0-mae:0.40335\n",
      "[82]\tvalidation_0-mae:0.40332\n",
      "[83]\tvalidation_0-mae:0.40325\n",
      "[84]\tvalidation_0-mae:0.40322\n",
      "[85]\tvalidation_0-mae:0.40322\n",
      "[86]\tvalidation_0-mae:0.40319\n",
      "[87]\tvalidation_0-mae:0.40303\n",
      "[88]\tvalidation_0-mae:0.40302\n",
      "[89]\tvalidation_0-mae:0.40299\n",
      "[90]\tvalidation_0-mae:0.40290\n",
      "[91]\tvalidation_0-mae:0.40286\n",
      "[92]\tvalidation_0-mae:0.40282\n",
      "[93]\tvalidation_0-mae:0.40279\n",
      "[94]\tvalidation_0-mae:0.40276\n",
      "[95]\tvalidation_0-mae:0.40275\n",
      "[96]\tvalidation_0-mae:0.40276\n",
      "[97]\tvalidation_0-mae:0.40272\n",
      "[98]\tvalidation_0-mae:0.40270\n",
      "[99]\tvalidation_0-mae:0.40268\n",
      "[100]\tvalidation_0-mae:0.40264\n",
      "[101]\tvalidation_0-mae:0.40266\n",
      "[102]\tvalidation_0-mae:0.40260\n",
      "[103]\tvalidation_0-mae:0.40257\n",
      "[104]\tvalidation_0-mae:0.40256\n",
      "[105]\tvalidation_0-mae:0.40252\n",
      "[106]\tvalidation_0-mae:0.40256\n",
      "[107]\tvalidation_0-mae:0.40253\n",
      "[108]\tvalidation_0-mae:0.40248\n",
      "[109]\tvalidation_0-mae:0.40240\n",
      "[110]\tvalidation_0-mae:0.40237\n",
      "[111]\tvalidation_0-mae:0.40231\n",
      "[112]\tvalidation_0-mae:0.40231\n",
      "[113]\tvalidation_0-mae:0.40225\n",
      "[114]\tvalidation_0-mae:0.40221\n",
      "[115]\tvalidation_0-mae:0.40218\n",
      "[116]\tvalidation_0-mae:0.40218\n",
      "[117]\tvalidation_0-mae:0.40212\n",
      "[118]\tvalidation_0-mae:0.40207\n",
      "[119]\tvalidation_0-mae:0.40207\n",
      "[120]\tvalidation_0-mae:0.40207\n",
      "[121]\tvalidation_0-mae:0.40197\n",
      "[122]\tvalidation_0-mae:0.40195\n",
      "[123]\tvalidation_0-mae:0.40192\n",
      "[124]\tvalidation_0-mae:0.40189\n",
      "[125]\tvalidation_0-mae:0.40187\n",
      "[126]\tvalidation_0-mae:0.40186\n",
      "[127]\tvalidation_0-mae:0.40174\n",
      "[128]\tvalidation_0-mae:0.40172\n",
      "[129]\tvalidation_0-mae:0.40173\n",
      "[130]\tvalidation_0-mae:0.40168\n",
      "[131]\tvalidation_0-mae:0.40164\n",
      "[132]\tvalidation_0-mae:0.40163\n",
      "[133]\tvalidation_0-mae:0.40162\n",
      "[134]\tvalidation_0-mae:0.40161\n",
      "[135]\tvalidation_0-mae:0.40160\n",
      "[136]\tvalidation_0-mae:0.40159\n",
      "[137]\tvalidation_0-mae:0.40157\n",
      "[138]\tvalidation_0-mae:0.40155\n",
      "[139]\tvalidation_0-mae:0.40152\n",
      "[140]\tvalidation_0-mae:0.40151\n",
      "[141]\tvalidation_0-mae:0.40152\n",
      "[142]\tvalidation_0-mae:0.40152\n",
      "[143]\tvalidation_0-mae:0.40151\n",
      "[144]\tvalidation_0-mae:0.40148\n",
      "[145]\tvalidation_0-mae:0.40147\n",
      "[146]\tvalidation_0-mae:0.40146\n",
      "[147]\tvalidation_0-mae:0.40144\n",
      "[148]\tvalidation_0-mae:0.40142\n",
      "[149]\tvalidation_0-mae:0.40139\n",
      "[150]\tvalidation_0-mae:0.40141\n",
      "[151]\tvalidation_0-mae:0.40126\n",
      "[152]\tvalidation_0-mae:0.40124\n",
      "[153]\tvalidation_0-mae:0.40120\n",
      "[154]\tvalidation_0-mae:0.40116\n",
      "[155]\tvalidation_0-mae:0.40117\n",
      "[156]\tvalidation_0-mae:0.40116\n",
      "[157]\tvalidation_0-mae:0.40114\n",
      "[158]\tvalidation_0-mae:0.40108\n",
      "[159]\tvalidation_0-mae:0.40107\n",
      "[160]\tvalidation_0-mae:0.40107\n",
      "[161]\tvalidation_0-mae:0.40101\n",
      "[162]\tvalidation_0-mae:0.40105\n",
      "[163]\tvalidation_0-mae:0.40104\n",
      "[164]\tvalidation_0-mae:0.40104\n",
      "[165]\tvalidation_0-mae:0.40107\n",
      "[166]\tvalidation_0-mae:0.40109\n",
      "[167]\tvalidation_0-mae:0.40109\n",
      "[168]\tvalidation_0-mae:0.40109\n",
      "[169]\tvalidation_0-mae:0.40109\n",
      "[170]\tvalidation_0-mae:0.40112\n",
      "[171]\tvalidation_0-mae:0.40113\n",
      "[172]\tvalidation_0-mae:0.40113\n",
      "[173]\tvalidation_0-mae:0.40114\n",
      "[174]\tvalidation_0-mae:0.40113\n",
      "[175]\tvalidation_0-mae:0.40111\n",
      "[176]\tvalidation_0-mae:0.40110\n",
      "[177]\tvalidation_0-mae:0.40108\n",
      "[178]\tvalidation_0-mae:0.40106\n",
      "[179]\tvalidation_0-mae:0.40105\n",
      "[180]\tvalidation_0-mae:0.40104\n",
      "[181]\tvalidation_0-mae:0.40107\n",
      "[182]\tvalidation_0-mae:0.40106\n",
      "[183]\tvalidation_0-mae:0.40104\n",
      "[184]\tvalidation_0-mae:0.40103\n",
      "[185]\tvalidation_0-mae:0.40098\n",
      "[186]\tvalidation_0-mae:0.40097\n",
      "[187]\tvalidation_0-mae:0.40097\n",
      "[188]\tvalidation_0-mae:0.40099\n",
      "[189]\tvalidation_0-mae:0.40097\n",
      "[190]\tvalidation_0-mae:0.40097\n",
      "[191]\tvalidation_0-mae:0.40094\n",
      "[192]\tvalidation_0-mae:0.40092\n",
      "[193]\tvalidation_0-mae:0.40089\n",
      "[194]\tvalidation_0-mae:0.40090\n",
      "[195]\tvalidation_0-mae:0.40088\n",
      "[196]\tvalidation_0-mae:0.40087\n",
      "[197]\tvalidation_0-mae:0.40087\n",
      "[198]\tvalidation_0-mae:0.40086\n",
      "[199]\tvalidation_0-mae:0.40084\n",
      "Train MAE: 2538.0117\n",
      "Test MAE: 2769.2964\n",
      "Train Log MAE: 0.3666\n",
      "Test Log MAE: 0.4008\n",
      "Final predictions saved to 'final_predictions_logspace.csv'.\n",
      "Model saved as 'model_logspace.pkl'\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from xgboost import XGBRegressor\n",
    "import pickle\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv(\"last.csv\")\n",
    "df = df.drop([\"Unnamed: 0\"], axis=1)\n",
    "\n",
    "# Step 1: Data Preparation\n",
    "print(\"Data shape:\", df.shape)\n",
    "\n",
    "# Define the columns to exclude and the features\n",
    "excluded_columns = ['timestamp', 'like_count', 'id_x', 'highlight_reel_count',\n",
    "                    'following_count', 'category_name', 'username',\n",
    "                    'count_caption', 'id_x']\n",
    "features = [col for col in df.columns if col not in excluded_columns and \"estimate\" not in col]\n",
    "X = df[features]\n",
    "y = df['like_count']\n",
    "\n",
    "# Step 2: Apply log transformation to the target variable (log-space training)\n",
    "y_log = np.log1p(y)  # Log transform the target variable\n",
    "\n",
    "# Step 3: Train-Test Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_log, test_size=0.24, random_state=15)\n",
    "\n",
    "# Step 4: Train the Model with Poisson Regression\n",
    "best_params = {\n",
    "    'learning_rate': 0.1,\n",
    "    'max_depth': 7,\n",
    "    'n_estimators': 200,\n",
    "    'subsample': 0.8,\n",
    "    'objective': 'reg:gamma',  \n",
    "    'seed': 8,\n",
    "    'eval_metric': 'mae'  # Set eval_metric here to avoid the warning\n",
    "}\n",
    "\n",
    "best_model = XGBRegressor(**best_params)\n",
    "best_model.fit(\n",
    "    X_train, y_train,\n",
    "    eval_set=[(X_test, y_test)],\n",
    "    verbose=True  # No need to include eval_metric here, it's already set in params\n",
    ")\n",
    "\n",
    "# Step 5: Predict in log space\n",
    "y_train_pred_log = best_model.predict(X_train)\n",
    "y_test_pred_log = best_model.predict(X_test)\n",
    "\n",
    "# Step 6: Convert predictions back to original scale (inverse log transformation)\n",
    "y_train_pred = np.expm1(y_train_pred_log)  # Inverse of log1p is expm1\n",
    "y_test_pred = np.expm1(y_test_pred_log)\n",
    "\n",
    "# Step 7: Post-process predictions to ensure non-negative values (Poisson regression guarantees non-negativity)\n",
    "y_train_pred = np.maximum(y_train_pred, 0)\n",
    "y_test_pred = np.maximum(y_test_pred, 0)\n",
    "\n",
    "# Calculate MAE for both train and test sets\n",
    "train_mae = mean_absolute_error(np.expm1(y_train), y_train_pred)  # Apply inverse log transformation to y_train\n",
    "test_mae = mean_absolute_error(np.expm1(y_test), y_test_pred)      # Apply inverse log transformation to y_test\n",
    "print(f\"Train MAE: {train_mae:.4f}\")\n",
    "print(f\"Test MAE: {test_mae:.4f}\")\n",
    "\n",
    "# Step 8: Calculate Log MAE (in log-space)\n",
    "train_log_mae = mean_absolute_error(y_train, y_train_pred_log)\n",
    "test_log_mae = mean_absolute_error(y_test, y_test_pred_log)\n",
    "print(f\"Train Log MAE: {train_log_mae:.4f}\")\n",
    "print(f\"Test Log MAE: {test_log_mae:.4f}\")\n",
    "\n",
    "# Step 9: Save Predictions (after inverse log transformation)\n",
    "df_test = df.loc[X_test.index]\n",
    "df_test['predicted_like_count'] = y_test_pred\n",
    "df_test.to_csv('final_predictions_logspace.csv', index=False)\n",
    "print(\"Final predictions saved to 'final_predictions_logspace.csv'.\")\n",
    "\n",
    "# Step 10: Save the Model\n",
    "with open('model_logspace.pkl', 'wb') as model_file:\n",
    "    pickle.dump(best_model, model_file)\n",
    "print(\"Model saved as 'model_logspace.pkl'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for negative predictions\n",
    "negative_predictions = df_test[df_test['predicted_like_count'] < 0]\n",
    "print(f\"Number of negative predictions: {len(negative_predictions)}\")\n",
    "\n",
    "# Optionally, display some negative predictions\n",
    "print(negative_predictions[['like_count', 'predicted_like_count']])\n",
    "\n",
    "# Check for NaN values in predictions\n",
    "nan_predictions = df_test[df_test['predicted_like_count'].isna()]\n",
    "print(f\"Number of NaN predictions: {len(nan_predictions)}\")\n",
    "\n",
    "# Optionally, display NaN predictions\n",
    "print(nan_predictions[['like_count', 'predicted_like_count']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "\n",
    "# Get feature importance\n",
    "importance = best_model.feature_importances_\n",
    "\n",
    "# Create a DataFrame for feature importance\n",
    "feature_importance_df = pd.DataFrame({\n",
    "    'feature': features,\n",
    "    'importance': importance\n",
    "}).sort_values(by='importance', ascending=False)\n",
    "\n",
    "print(\"Feature Importance:\")\n",
    "print(feature_importance_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# List of columns\n",
    "cols = ['like_count', 'predicted_like_count', 'estimate_likes_media_type']\n",
    "\n",
    "# Create a function to compute MAE\n",
    "def mae(true_values, predicted_values):\n",
    "    return np.mean(np.abs(true_values - predicted_values))\n",
    "\n",
    "# Create a dictionary to store the MAE results\n",
    "mae_results = {}\n",
    "\n",
    "# Compute MAE for each pair of columns\n",
    "for i in range(3):\n",
    "    for j in range(3):\n",
    "        if i != j:\n",
    "            mae_value = mae(df_test[cols[i]], df_test[cols[j]])\n",
    "            mae_results[f'{cols[i]} vs {cols[j]}'] = mae_value\n",
    "\n",
    "# Print the MAE results\n",
    "for pair, mae_value in mae_results.items():\n",
    "    print(f'MAE between {pair}: {mae_value:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Function to compute MAE\n",
    "def mae(true_values, predicted_values):\n",
    "    return np.mean(np.abs(true_values - predicted_values))\n",
    "\n",
    "# Function to get largest contributors\n",
    "def get_largest_contributors(true_values, predicted_values, num_contributors=5):\n",
    "    # Compute absolute errors for each row\n",
    "    absolute_errors = np.abs(true_values - predicted_values)\n",
    "    \n",
    "    # Get the rows with the largest errors\n",
    "    largest_contributors = absolute_errors.nlargest(num_contributors)\n",
    "    \n",
    "    return largest_contributors\n",
    "\n",
    "# Compute MAE for like_count and predicted_like_count\n",
    "mae_value = mae(df_test['like_count'], df_test['predicted_like_count'])\n",
    "\n",
    "# Get the largest contributors\n",
    "largest_contributors = get_largest_contributors(df_test['like_count'], df_test['predicted_like_count'])\n",
    "\n",
    "# Output the results\n",
    "print(f'MAE between like_count and predicted_like_count: {mae_value:.4f}')\n",
    "print(\"\\nLargest Contributors (Top 5 rows):\")\n",
    "print(largest_contributors)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test['residual'] = np.abs(df_test['like_count'] - df_test['predicted_like_count'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.sort_values('residual', ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Plot residuals to see how they're distributed\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(df_test['residual'], bins=50, color='blue', kde=True)\n",
    "plt.title(\"Distribution of Residuals (Prediction Error)\")\n",
    "plt.xlabel(\"Residual\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.show()\n",
    "\n",
    "# Check for large residuals (i.e., cases where the model has a large error)\n",
    "large_residuals = df_test[df_test['residual'].abs() > 1e6]  # Adjust threshold as necessary\n",
    "print(\"Data points with large residuals (model errors):\")\n",
    "print(large_residuals[['like_count', 'predicted_like_count', 'residual']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import xgboost as xgb\n",
    "\n",
    "# 1. **Feature Importance using XGBoost's built-in method**\n",
    "# Visualize the feature importance (it ranks features based on their impact on the model)\n",
    "plt.figure(figsize=(10, 6))\n",
    "xgb.plot_importance(best_model, importance_type='weight', max_num_features=10, height=0.8)\n",
    "plt.title('Top 10 Feature Importance by Weight')\n",
    "plt.show()\n",
    "\n",
    "# You can also use 'gain' or 'cover' for other types of importance\n",
    "plt.figure(figsize=(10, 6))\n",
    "xgb.plot_importance(best_model, importance_type='gain', max_num_features=10, height=0.8)\n",
    "plt.title('Top 10 Feature Importance by Gain')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a threshold for 'celebrity' (top 5% by like_count)\n",
    "celebrity_threshold = df_test['like_count'].quantile(0.999)\n",
    "\n",
    "# Filter out celebrity data (those with high like_count)\n",
    "celebrity_data = df_test[df_test['like_count'] >= celebrity_threshold]\n",
    "\n",
    "# Check residuals for celebrity data\n",
    "celebrity_residuals = celebrity_data[['like_count', 'predicted_like_count', 'residual']]\n",
    "print(celebrity_residuals)\n",
    "\n",
    "# Visualize residuals for celebrities\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.scatterplot(x=celebrity_residuals.index, y=celebrity_residuals['residual'], color='red')\n",
    "plt.title(\"Residuals for Celebrity Data (High Like Count)\")\n",
    "plt.xlabel(\"Index\")\n",
    "plt.ylabel(\"Residual (Prediction Error)\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
